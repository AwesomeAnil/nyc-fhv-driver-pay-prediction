# ğŸš– Predicting High-Volume NYC Taxi Trips  
*From Exploratory Analysis to Predictive Modeling*  

[![Python](https://img.shields.io/badge/Python-3.9%2B-blue)]()  
[![Machine Learning](https://img.shields.io/badge/ML-LightGBM%20%7C%20Linear%20Regression-green)]()  
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)]()  

---

## ğŸ“‘ Table of Contents  
- ğŸ¯ Executive Summary  
- ğŸ“Š Key Results at a Glance  
- ğŸ’¼ Business Value & Use Cases  
- ğŸ” EDA Highlights  
- ğŸ“ˆ Linear Regression (OLS) Model  
- ğŸŒ² LightGBM Model  
- ğŸ§ª Predictions on New Data  
- ğŸª Caveats & Scales  
- ğŸš€ Recommendations & Next Steps  
- ğŸ—‚ï¸ Repo Layout & How to Reproduce  
- âœ¨ Closing Thoughts  

---

## ğŸ¯ Executive Summary  
This repo contains an end-to-end ML pipeline built to predict **driver pay** for NYC high-volume taxi trips.  

- **Business case:** Optimize driver allocation & reduce idle time.  
- **Technical case:** Rigorous workflow from EDA â†’ Feature Engineering â†’ Model Training â†’ Predictions.  
- **Investor case:** Predictive intelligence drives efficiency in ride-hailing, logistics, and mobility â€” a **multi-billion-dollar opportunity**.  

---

## ğŸ“Š Key Results at a Glance  

| Stage | Model | MAE | MSE | RMSE | RÂ² | Source |
|---|---:|---:|---:|---:|---:|---|
| Training | Linear Regression (OLS) | â€” | â€” | â€” | **0.901** | `03_Model_Training` |
| Predictions (new data) | Linear Regression | **0.16** | **0.05** | **0.21** | **0.90** | `04_New_data_Predictions` |
| Training | LightGBM | â€” | â€” | **3.41** | ~0.88 | `03a_Model_Training_Trees` |
| Eval | LightGBM | **1.56** | â€” | **3.41** | **0.9676** | `03a_Model_Training_Trees` |
| Predictions (new data) | LightGBM | **1.44** | **14.54** | **3.81** | **0.96** | `04a_New_Data_Predictions` |

âš ï¸ *Note: Linear regression metrics are on a transformed target; LightGBM metrics are in original currency units (USD). See [ğŸª Caveats & Scales](#-caveats--scales).*  

---

## ğŸ’¼ Business Value & Use Cases  
- **Operational forecasting:** Optimize driver allocation & payouts.  
- **Pricing & margin:** Inform commission/incentive strategies.  
- **Fraud detection:** Spot anomalies in payouts.  
- **Investor story:** Demonstrates production-ready ML with measurable ROI.  

---

## ğŸ” EDA Highlights  
- Target `driver_pay` (raw): mean â‰ˆ 21.13, count â‰ˆ 2.5M trips.  
- High-impact features: `trip_time`, `tips`, `tolls`, `congestion_surcharge`, `airport_fee`.  
- Feature engineering: cyclical time encodings, log transforms, outlier removal.  

ğŸ“Š *See [`02_EDA_Feature_Engg.ipynb`](02_EDA_Feature_Engg.ipynb).*  

---

## ğŸ“ˆ Linear Regression (OLS) Model  
- **RÂ² = 0.901** (training, OLS summary).  
- Predictions on new data:  
  - MAE = 0.16  
  - RMSE = 0.21  
  - RÂ² = 0.90  

âœ… Interpretable baseline, useful for explainability.  
ğŸ“Š *See [`03_Model_Training.ipynb`](03_Model_Training.ipynb).*  

---

## ğŸŒ² LightGBM Model  
- Training best valid RMSE â‰ˆ **3.41**.  
- Evaluation (test set): MAE â‰ˆ 1.56, RMSE â‰ˆ 3.41, RÂ² â‰ˆ 0.9676.  
- Predictions on new data: MAE = 1.44, RMSE = 3.81, RÂ² = 0.96.  

âœ… Outperforms Linear Regression; robust to non-linearities.  
ğŸ“Š *See [`03a_Model_Training_Trees.ipynb`](03a_Model_Training_Trees.ipynb).*  

---

## ğŸ§ª Predictions on New Data  

| Model | Predicted | Actual | Notes |
|-------|-----------|--------|-------|
| Linear Regression | 14.2 | 15.0 | Baseline |
| LightGBM | 13.8 | 14.0 | More accurate |

ğŸ“Š *See prediction notebooks for detailed plots:*  
- [`04_New_data_Predictions.ipynb`](04_New_data_Predictions.ipynb)  
- [`04a_New_Data_Predictions.ipynb`](04a_New_Data_Predictions.ipynb)  

---

## ğŸª Caveats & Scales  
- **Linear Regression** results (RMSE = 0.21) are reported on a **transformed target**.  
- **LightGBM** results (RMSE â‰ˆ 3.4â€“3.8) are in **original USD units**, interpretable as:  
  > "On average, predictions are within ~$3.80 of actual driver pay."  
- âœ… For apples-to-apples comparison: apply the same inverse-transform to both models and recompute metrics.  

---

## ğŸš€ Recommendations & Next Steps  
1. Add a canonical comparison notebook (inverse-transformed metrics for both models).  
2. Integrate SHAP explainability for LightGBM.  
3. Package model inference with FastAPI for deployment.  
4. Build a one-page investor pitch: Problem â†’ Solution â†’ KPI â†’ ROI.  

---

## ğŸ—‚ï¸ Repo Layout & How to Reproduce  

```text
â”œâ”€â”€ 01_Loading_Staging.ipynb        # Data ingestion and staging for EDA
â”œâ”€â”€ 02_EDA_Feature_Engg.ipynb       # EDA & feature engineering  
â”œâ”€â”€ 03_Model_Training.ipynb         # Linear Regression (OLS)  
â”œâ”€â”€ 03a_Model_Training_Trees.ipynb  # LightGBM  
â”œâ”€â”€ 04_New_data_Predictions.ipynb   # Linear Regression predictions  
â”œâ”€â”€ 04a_New_Data_Predictions.ipynb  # LightGBM predictions  
â”œâ”€â”€ CONFIG_ENV.md                   # Environment & config guide  
â”œâ”€â”€ requirements.txt                # Pinned dependencies  

---

**Setup:**  
```bash
git clone https://github.com/<your-username>/<your-repo>.git
cd <your-repo>
python -m venv venv
source venv/bin/activate   # macOS/Linux
venv\Scripts\activate      # Windows
pip install -r requirements.txt

Run notebooks in order for full pipeline reproduction.
